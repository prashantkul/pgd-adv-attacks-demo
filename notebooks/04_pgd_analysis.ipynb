{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Advanced PGD Analysis & Defense Mechanisms\n",
    "\n",
    "## Understanding Attack Patterns and Developing Defenses\n",
    "\n",
    "In this final notebook of our PGD series, we'll dive deep into:\n",
    "- Attack pattern analysis and visualization\n",
    "- Transferability of adversarial examples across models\n",
    "- Defense mechanisms and their effectiveness\n",
    "- Real-world implications and case studies\n",
    "\n",
    "### Learning Objectives:\n",
    "- Analyze how PGD attacks exploit model vulnerabilities\n",
    "- Understand adversarial example transferability\n",
    "- Implement and evaluate defense strategies\n",
    "- Explore gradient masking and detection methods\n",
    "- Discuss ethical implications and responsible disclosure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision matplotlib numpy seaborn tqdm ipywidgets scikit-learn pillow opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('../src')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom tqdm import tqdm\nimport pandas as pd\nfrom ipywidgets import interact, FloatSlider, IntSlider, Dropdown, Checkbox\nfrom IPython.display import display, HTML\nimport cv2\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import our custom modules\nfrom attacks.wrappers import PGDAttack, FGSM, IterativeFGSM, MomentumFGSM\nfrom models.load_models import load_resnet18, load_vgg16, load_densenet121\nfrom utils.visualization import plot_adversarial_examples, plot_attack_comparison\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Check device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Set plotting style\nplt.style.use('default')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Prepare Analysis Dataset\n",
    "\n",
    "We'll load multiple models and create a diverse test set for comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load different model architectures\n",
    "models = {\n",
    "    'ResNet18': load_resnet18(device),\n",
    "    'VGG16': load_vgg16(device),\n",
    "    'DenseNet121': load_densenet121(device)\n",
    "}\n",
    "\n",
    "print(\"Loaded models for analysis:\")\n",
    "for name, model in models.items():\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"- {name}: {total_params:,} parameters\")\n",
    "\n",
    "# Load and prepare test dataset\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='../data', train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "# Create analysis subset with balanced classes\n",
    "analysis_indices = []\n",
    "samples_per_class = 10\n",
    "\n",
    "class_counts = {i: 0 for i in range(10)}\n",
    "for idx, (_, label) in enumerate(test_dataset):\n",
    "    if class_counts[label] < samples_per_class:\n",
    "        analysis_indices.append(idx)\n",
    "        class_counts[label] += 1\n",
    "    if all(count >= samples_per_class for count in class_counts.values()):\n",
    "        break\n",
    "\n",
    "analysis_subset = torch.utils.data.Subset(test_dataset, analysis_indices)\n",
    "analysis_loader = torch.utils.data.DataLoader(\n",
    "    analysis_subset, batch_size=1, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nAnalysis dataset: {len(analysis_subset)} images ({samples_per_class} per class)\")\n",
    "\n",
    "# CIFAR-10 class names\n",
    "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attack Pattern Analysis\n",
    "\n",
    "Let's analyze how PGD attacks affect different types of images and identify common patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attack_patterns(model, model_name, epsilon=0.08):\n",
    "    \"\"\"\n",
    "    Analyze patterns in successful PGD attacks.\n",
    "    \"\"\"\n",
    "    pgd_attack = PGDAttack(\n",
    "        model=model,\n",
    "        epsilon=epsilon,\n",
    "        alpha=epsilon/4,\n",
    "        steps=20,\n",
    "        random_start=True,\n",
    "        norm='inf'\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    attack_data = {'successful': [], 'failed': []}\n",
    "    \n",
    "    for images, labels in tqdm(analysis_loader, desc=f\"Analyzing {model_name} patterns\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Get original predictions\n",
    "        with torch.no_grad():\n",
    "            orig_outputs = model(images)\n",
    "            orig_probs = torch.softmax(orig_outputs, dim=1)\n",
    "            orig_pred = orig_outputs.argmax(dim=1).item()\n",
    "            orig_confidence = orig_probs.max().item()\n",
    "            \n",
    "            # Get top-3 predictions\n",
    "            top3_probs, top3_preds = torch.topk(orig_probs, 3)\n",
    "            top3_confidence_gap = top3_probs[0, 0].item() - top3_probs[0, 1].item()\n",
    "        \n",
    "        # Generate adversarial example\n",
    "        adv_images = pgd_attack(images, labels)\n",
    "        \n",
    "        # Get adversarial predictions\n",
    "        with torch.no_grad():\n",
    "            adv_outputs = model(adv_images)\n",
    "            adv_probs = torch.softmax(adv_outputs, dim=1)\n",
    "            adv_pred = adv_outputs.argmax(dim=1).item()\n",
    "            adv_confidence = adv_probs.max().item()\n",
    "        \n",
    "        # Calculate perturbation statistics\n",
    "        perturbation = adv_images - images\n",
    "        linf_norm = torch.norm(perturbation, p=float('inf')).item()\n",
    "        l2_norm = torch.norm(perturbation, p=2).item()\n",
    "        l1_norm = torch.norm(perturbation, p=1).item()\n",
    "        \n",
    "        # Analyze perturbation distribution\n",
    "        pert_std = perturbation.std().item()\n",
    "        pert_mean = perturbation.mean().item()\n",
    "        pert_sparsity = (torch.abs(perturbation) < 0.01).float().mean().item()\n",
    "        \n",
    "        # Image characteristics\n",
    "        image_brightness = images.mean().item()\n",
    "        image_contrast = images.std().item()\n",
    "        \n",
    "        attack_successful = orig_pred != adv_pred\n",
    "        confidence_drop = orig_confidence - adv_confidence\n",
    "        \n",
    "        result = {\n",
    "            'model': model_name,\n",
    "            'true_class': labels.item(),\n",
    "            'true_class_name': cifar10_classes[labels.item()],\n",
    "            'orig_pred': orig_pred,\n",
    "            'orig_pred_name': cifar10_classes[orig_pred],\n",
    "            'adv_pred': adv_pred,\n",
    "            'adv_pred_name': cifar10_classes[adv_pred],\n",
    "            'attack_successful': attack_successful,\n",
    "            'orig_confidence': orig_confidence,\n",
    "            'adv_confidence': adv_confidence,\n",
    "            'confidence_drop': confidence_drop,\n",
    "            'top3_confidence_gap': top3_confidence_gap,\n",
    "            'linf_norm': linf_norm,\n",
    "            'l2_norm': l2_norm,\n",
    "            'l1_norm': l1_norm,\n",
    "            'pert_std': pert_std,\n",
    "            'pert_mean': pert_mean,\n",
    "            'pert_sparsity': pert_sparsity,\n",
    "            'image_brightness': image_brightness,\n",
    "            'image_contrast': image_contrast\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Store data for visualization\n",
    "        data_point = {\n",
    "            'image': images.cpu(),\n",
    "            'adv_image': adv_images.cpu(),\n",
    "            'perturbation': perturbation.cpu(),\n",
    "            'result': result\n",
    "        }\n",
    "        \n",
    "        if attack_successful:\n",
    "            attack_data['successful'].append(data_point)\n",
    "        else:\n",
    "            attack_data['failed'].append(data_point)\n",
    "    \n",
    "    return results, attack_data\n",
    "\n",
    "# Analyze patterns for ResNet18\n",
    "resnet_results, resnet_attack_data = analyze_attack_patterns(models['ResNet18'], 'ResNet18')\n",
    "pattern_df = pd.DataFrame(resnet_results)\n",
    "\n",
    "print(f\"\\nPattern Analysis Results for ResNet18:\")\n",
    "print(f\"Total attacks: {len(pattern_df)}\")\n",
    "print(f\"Successful: {pattern_df['attack_successful'].sum()}\")\n",
    "print(f\"Success rate: {pattern_df['attack_successful'].mean():.3f}\")\n",
    "print(f\"\\nSuccessful attacks: {len(resnet_attack_data['successful'])} examples\")\n",
    "print(f\"Failed attacks: {len(resnet_attack_data['failed'])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attack patterns\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "# Success rate by true class\n",
    "class_success = pattern_df.groupby('true_class_name')['attack_successful'].mean().sort_values(ascending=False)\n",
    "class_success.plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "axes[0,0].set_title('Attack Success Rate by True Class')\n",
    "axes[0,0].set_ylabel('Success Rate')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confidence vs Success\n",
    "successful = pattern_df[pattern_df['attack_successful']]\n",
    "failed = pattern_df[~pattern_df['attack_successful']]\n",
    "\n",
    "axes[0,1].scatter(successful['orig_confidence'], successful['confidence_drop'], \n",
    "                  alpha=0.6, label='Successful', color='red', s=30)\n",
    "axes[0,1].scatter(failed['orig_confidence'], failed['confidence_drop'], \n",
    "                  alpha=0.6, label='Failed', color='blue', s=30)\n",
    "axes[0,1].set_xlabel('Original Confidence')\n",
    "axes[0,1].set_ylabel('Confidence Drop')\n",
    "axes[0,1].set_title('Confidence Drop vs Original Confidence')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Top-3 confidence gap vs success\n",
    "axes[0,2].boxplot([successful['top3_confidence_gap'], failed['top3_confidence_gap']], \n",
    "                  labels=['Successful', 'Failed'])\n",
    "axes[0,2].set_title('Top-3 Confidence Gap Distribution')\n",
    "axes[0,2].set_ylabel('Confidence Gap')\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# Perturbation norms comparison\n",
    "axes[1,0].scatter(successful['l2_norm'], successful['linf_norm'], \n",
    "                  alpha=0.6, label='Successful', color='red', s=30)\n",
    "axes[1,0].scatter(failed['l2_norm'], failed['linf_norm'], \n",
    "                  alpha=0.6, label='Failed', color='blue', s=30)\n",
    "axes[1,0].set_xlabel('L2 Norm')\n",
    "axes[1,0].set_ylabel('L∞ Norm')\n",
    "axes[1,0].set_title('Perturbation Norms')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Image characteristics vs success\n",
    "axes[1,1].scatter(successful['image_brightness'], successful['image_contrast'], \n",
    "                  alpha=0.6, label='Successful', color='red', s=30)\n",
    "axes[1,1].scatter(failed['image_brightness'], failed['image_contrast'], \n",
    "                  alpha=0.6, label='Failed', color='blue', s=30)\n",
    "axes[1,1].set_xlabel('Image Brightness')\n",
    "axes[1,1].set_ylabel('Image Contrast')\n",
    "axes[1,1].set_title('Image Characteristics')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Perturbation characteristics\n",
    "axes[1,2].scatter(successful['pert_std'], successful['pert_sparsity'], \n",
    "                  alpha=0.6, label='Successful', color='red', s=30)\n",
    "axes[1,2].scatter(failed['pert_std'], failed['pert_sparsity'], \n",
    "                  alpha=0.6, label='Failed', color='blue', s=30)\n",
    "axes[1,2].set_xlabel('Perturbation Std')\n",
    "axes[1,2].set_ylabel('Perturbation Sparsity')\n",
    "axes[1,2].set_title('Perturbation Characteristics')\n",
    "axes[1,2].legend()\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "# Attack confusion matrix\n",
    "attack_confusion = confusion_matrix(\n",
    "    pattern_df['orig_pred'], \n",
    "    pattern_df['adv_pred'],\n",
    "    labels=range(10)\n",
    ")\n",
    "im = axes[2,0].imshow(attack_confusion, cmap='Blues')\n",
    "axes[2,0].set_title('Attack Confusion Matrix\\n(Original → Adversarial)')\n",
    "axes[2,0].set_xlabel('Adversarial Prediction')\n",
    "axes[2,0].set_ylabel('Original Prediction')\n",
    "axes[2,0].set_xticks(range(10))\n",
    "axes[2,0].set_yticks(range(10))\n",
    "axes[2,0].set_xticklabels([c[:4] for c in cifar10_classes], rotation=45)\n",
    "axes[2,0].set_yticklabels([c[:4] for c in cifar10_classes])\n",
    "plt.colorbar(im, ax=axes[2,0], shrink=0.8)\n",
    "\n",
    "# Most common attack transitions\n",
    "transitions = pattern_df[pattern_df['attack_successful']]\n",
    "transition_counts = transitions.groupby(['orig_pred_name', 'adv_pred_name']).size().sort_values(ascending=False)\n",
    "top_transitions = transition_counts.head(10)\n",
    "\n",
    "transition_labels = [f\"{orig}→{adv}\" for orig, adv in top_transitions.index]\n",
    "axes[2,1].barh(range(len(top_transitions)), top_transitions.values, color='lightcoral')\n",
    "axes[2,1].set_yticks(range(len(top_transitions)))\n",
    "axes[2,1].set_yticklabels(transition_labels)\n",
    "axes[2,1].set_xlabel('Frequency')\n",
    "axes[2,1].set_title('Most Common Attack Transitions')\n",
    "axes[2,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Success rate vs image complexity (using contrast as proxy)\n",
    "pattern_df['complexity_bin'] = pd.cut(pattern_df['image_contrast'], bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "complexity_success = pattern_df.groupby('complexity_bin')['attack_successful'].agg(['mean', 'count'])\n",
    "complexity_success['mean'].plot(kind='bar', ax=axes[2,2], color='lightgreen')\n",
    "axes[2,2].set_title('Attack Success vs Image Complexity')\n",
    "axes[2,2].set_ylabel('Success Rate')\n",
    "axes[2,2].set_xlabel('Image Complexity (Contrast)')\n",
    "axes[2,2].tick_params(axis='x', rotation=45)\n",
    "axes[2,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key insights\n",
    "print(\"\\n=== KEY ATTACK PATTERNS IDENTIFIED ===\")\n",
    "print(f\"\\n1. CLASS VULNERABILITY RANKING:\")\n",
    "for i, (class_name, success_rate) in enumerate(class_success.items()):\n",
    "    print(f\"   {i+1}. {class_name}: {success_rate:.3f} success rate\")\n",
    "\n",
    "print(f\"\\n2. CONFIDENCE ANALYSIS:\")\n",
    "avg_orig_conf_success = successful['orig_confidence'].mean()\n",
    "avg_orig_conf_failed = failed['orig_confidence'].mean()\n",
    "print(f\"   - Average original confidence (successful attacks): {avg_orig_conf_success:.3f}\")\n",
    "print(f\"   - Average original confidence (failed attacks): {avg_orig_conf_failed:.3f}\")\n",
    "print(f\"   - Difference: {avg_orig_conf_failed - avg_orig_conf_success:.3f}\")\n",
    "\n",
    "print(f\"\\n3. MOST COMMON ATTACK TRANSITIONS:\")\n",
    "for i, ((orig, adv), count) in enumerate(top_transitions.head(5).items()):\n",
    "    print(f\"   {i+1}. {orig} → {adv}: {count} times\")\n",
    "\n",
    "print(f\"\\n4. IMAGE COMPLEXITY INSIGHTS:\")\n",
    "for complexity, stats in complexity_success.iterrows():\n",
    "    print(f\"   {complexity} complexity: {stats['mean']:.3f} success rate ({stats['count']} images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adversarial Transferability Analysis\n",
    "\n",
    "Let's examine how adversarial examples transfer across different model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_transferability(source_models, target_models, num_samples=30):\n",
    "    \"\"\"\n",
    "    Analyze transferability of adversarial examples across models.\n",
    "    \"\"\"\n",
    "    # Create sample subset\n",
    "    sample_indices = torch.randperm(len(analysis_subset))[:num_samples]\n",
    "    sample_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.Subset(analysis_subset, sample_indices), \n",
    "        batch_size=1, shuffle=False\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for source_name, source_model in tqdm(source_models.items(), desc=\"Testing transferability\"):\n",
    "        # Create PGD attack on source model\n",
    "        pgd_attack = PGDAttack(\n",
    "            model=source_model,\n",
    "            epsilon=0.08,\n",
    "            alpha=0.02,\n",
    "            steps=20,\n",
    "            random_start=True,\n",
    "            norm='inf'\n",
    "        )\n",
    "        \n",
    "        for images, labels in sample_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Generate adversarial example using source model\n",
    "            adv_images = pgd_attack(images, labels)\n",
    "            \n",
    "            # Get original predictions from all models\n",
    "            orig_preds = {}\n",
    "            orig_confidences = {}\n",
    "            \n",
    "            for target_name, target_model in target_models.items():\n",
    "                with torch.no_grad():\n",
    "                    outputs = target_model(images)\n",
    "                    probs = torch.softmax(outputs, dim=1)\n",
    "                    orig_preds[target_name] = outputs.argmax(dim=1).item()\n",
    "                    orig_confidences[target_name] = probs.max().item()\n",
    "            \n",
    "            # Test adversarial example on all target models\n",
    "            for target_name, target_model in target_models.items():\n",
    "                with torch.no_grad():\n",
    "                    adv_outputs = target_model(adv_images)\n",
    "                    adv_probs = torch.softmax(adv_outputs, dim=1)\n",
    "                    adv_pred = adv_outputs.argmax(dim=1).item()\n",
    "                    adv_confidence = adv_probs.max().item()\n",
    "                \n",
    "                # Check if attack transferred\n",
    "                attack_transferred = orig_preds[target_name] != adv_pred\n",
    "                confidence_drop = orig_confidences[target_name] - adv_confidence\n",
    "                \n",
    "                results.append({\n",
    "                    'source_model': source_name,\n",
    "                    'target_model': target_name,\n",
    "                    'true_class': labels.item(),\n",
    "                    'orig_pred': orig_preds[target_name],\n",
    "                    'adv_pred': adv_pred,\n",
    "                    'attack_transferred': attack_transferred,\n",
    "                    'orig_confidence': orig_confidences[target_name],\n",
    "                    'adv_confidence': adv_confidence,\n",
    "                    'confidence_drop': confidence_drop,\n",
    "                    'same_model': source_name == target_name\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Analyze transferability\n",
    "transfer_results = analyze_transferability(models, models)\n",
    "\n",
    "print(\"Transferability Analysis Results:\")\n",
    "print(f\"Total combinations tested: {len(transfer_results)}\")\n",
    "print(f\"\\nOverall transfer success rate: {transfer_results['attack_transferred'].mean():.3f}\")\n",
    "print(f\"Same-model success rate: {transfer_results[transfer_results['same_model']]['attack_transferred'].mean():.3f}\")\n",
    "print(f\"Cross-model transfer rate: {transfer_results[~transfer_results['same_model']]['attack_transferred'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transferability results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Transfer matrix (source → target)\n",
    "transfer_matrix = transfer_results.groupby(['source_model', 'target_model'])['attack_transferred'].mean().unstack()\n",
    "sns.heatmap(transfer_matrix, annot=True, fmt='.3f', cmap='RdYlBu_r', ax=axes[0,0])\n",
    "axes[0,0].set_title('Adversarial Transferability Matrix\\n(Source → Target Models)')\n",
    "axes[0,0].set_xlabel('Target Model')\n",
    "axes[0,0].set_ylabel('Source Model')\n",
    "\n",
    "# Average transfer rates by source model\n",
    "source_transfer = transfer_results.groupby('source_model')['attack_transferred'].mean().sort_values(ascending=False)\n",
    "source_transfer.plot(kind='bar', ax=axes[0,1], color='lightcoral')\n",
    "axes[0,1].set_title('Average Transfer Rate by Source Model')\n",
    "axes[0,1].set_ylabel('Transfer Success Rate')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Average vulnerability by target model\n",
    "target_vuln = transfer_results.groupby('target_model')['attack_transferred'].mean().sort_values(ascending=False)\n",
    "target_vuln.plot(kind='bar', ax=axes[1,0], color='lightblue')\n",
    "axes[1,0].set_title('Average Vulnerability by Target Model')\n",
    "axes[1,0].set_ylabel('Transfer Success Rate')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confidence drop comparison\n",
    "same_model_drops = transfer_results[transfer_results['same_model']]['confidence_drop']\n",
    "cross_model_drops = transfer_results[~transfer_results['same_model']]['confidence_drop']\n",
    "\n",
    "axes[1,1].hist(same_model_drops, alpha=0.7, label='Same Model', bins=20, color='red')\n",
    "axes[1,1].hist(cross_model_drops, alpha=0.7, label='Cross Model', bins=20, color='blue')\n",
    "axes[1,1].set_title('Confidence Drop Distribution')\n",
    "axes[1,1].set_xlabel('Confidence Drop')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed transferability analysis\n",
    "print(\"\\n=== DETAILED TRANSFERABILITY ANALYSIS ===\")\n",
    "\n",
    "print(\"\\n1. MODEL-SPECIFIC TRANSFER RATES:\")\n",
    "for source in models.keys():\n",
    "    print(f\"\\n   {source} as source model:\")\n",
    "    source_data = transfer_results[transfer_results['source_model'] == source]\n",
    "    for target in models.keys():\n",
    "        target_data = source_data[source_data['target_model'] == target]\n",
    "        success_rate = target_data['attack_transferred'].mean()\n",
    "        avg_conf_drop = target_data['confidence_drop'].mean()\n",
    "        if source == target:\n",
    "            print(f\"     → {target}: {success_rate:.3f} (same model, conf drop: {avg_conf_drop:.3f})\")\n",
    "        else:\n",
    "            print(f\"     → {target}: {success_rate:.3f} (transfer, conf drop: {avg_conf_drop:.3f})\")\n",
    "\n",
    "print(\"\\n2. CROSS-MODEL TRANSFER SUMMARY:\")\n",
    "cross_transfers = transfer_results[~transfer_results['same_model']]\n",
    "transfer_pairs = cross_transfers.groupby(['source_model', 'target_model'])['attack_transferred'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"   Best transfer pairs:\")\n",
    "for (source, target), rate in transfer_pairs.head(5).items():\n",
    "    print(f\"     {source} → {target}: {rate:.3f}\")\n",
    "\n",
    "print(\"\\n   Worst transfer pairs:\")\n",
    "for (source, target), rate in transfer_pairs.tail(5).items():\n",
    "    print(f\"     {source} → {target}: {rate:.3f}\")\n",
    "\n",
    "print(\"\\n3. ROBUSTNESS RANKING (by vulnerability to transferred attacks):\")\n",
    "cross_target_vuln = cross_transfers.groupby('target_model')['attack_transferred'].mean().sort_values()\n",
    "for i, (model, vuln) in enumerate(cross_target_vuln.items()):\n",
    "    print(f\"   {i+1}. {model}: {vuln:.3f} transfer vulnerability (lower is better)\")\n",
    "\n",
    "print(\"\\n4. ATTACK GENERALIZATION RANKING (by transfer generation ability):\")\n",
    "cross_source_gen = cross_transfers.groupby('source_model')['attack_transferred'].mean().sort_values(ascending=False)\n",
    "for i, (model, gen) in enumerate(cross_source_gen.items()):\n",
    "    print(f\"   {i+1}. {model}: {gen:.3f} transfer generation (higher means more generalizable attacks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defense Mechanisms Implementation and Evaluation\n",
    "\n",
    "Now let's implement and test several defense strategies against PGD attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialDefenses:\n",
    "    \"\"\"\n",
    "    Collection of adversarial defense mechanisms.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def gaussian_noise_defense(images, std=0.05):\n",
    "        \"\"\"\n",
    "        Add Gaussian noise as a preprocessing defense.\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(images) * std\n",
    "        return torch.clamp(images + noise, 0, 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def median_filter_defense(images, kernel_size=3):\n",
    "        \"\"\"\n",
    "        Apply median filtering as preprocessing defense.\n",
    "        \"\"\"\n",
    "        defended_images = images.clone()\n",
    "        \n",
    "        for i in range(images.shape[0]):\n",
    "            for c in range(images.shape[1]):\n",
    "                img_np = images[i, c].cpu().numpy()\n",
    "                img_filtered = cv2.medianBlur((img_np * 255).astype(np.uint8), kernel_size)\n",
    "                defended_images[i, c] = torch.from_numpy(img_filtered / 255.0).to(images.device)\n",
    "        \n",
    "        return defended_images\n",
    "    \n",
    "    @staticmethod\n",
    "    def bit_depth_reduction(images, bits=4):\n",
    "        \"\"\"\n",
    "        Reduce bit depth as a defense mechanism.\n",
    "        \"\"\"\n",
    "        scale = 2 ** bits - 1\n",
    "        quantized = torch.round(images * scale) / scale\n",
    "        return torch.clamp(quantized, 0, 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def jpeg_compression_defense(images, quality=75):\n",
    "        \"\"\"\n",
    "        Simulate JPEG compression defense.\n",
    "        \"\"\"\n",
    "        defended_images = images.clone()\n",
    "        \n",
    "        for i in range(images.shape[0]):\n",
    "            # Convert to PIL format\n",
    "            img_np = (images[i].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            \n",
    "            # Simulate JPEG compression by reducing precision\n",
    "            # This is a simplified version - real JPEG would involve DCT\n",
    "            compressed = np.round(img_np / (101 - quality)) * (101 - quality)\n",
    "            compressed = np.clip(compressed, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            defended_images[i] = torch.from_numpy(compressed / 255.0).permute(2, 0, 1).to(images.device)\n",
    "        \n",
    "        return defended_images\n",
    "    \n",
    "    @staticmethod\n",
    "    def ensemble_defense(images, models, weights=None):\n",
    "        \"\"\"\n",
    "        Ensemble defense using multiple models.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = [1.0 / len(models)] * len(models)\n",
    "        \n",
    "        ensemble_output = None\n",
    "        \n",
    "        for model, weight in zip(models, weights):\n",
    "            with torch.no_grad():\n",
    "                output = model(images)\n",
    "                if ensemble_output is None:\n",
    "                    ensemble_output = weight * output\n",
    "                else:\n",
    "                    ensemble_output += weight * output\n",
    "        \n",
    "        return ensemble_output\n",
    "    \n",
    "    @staticmethod\n",
    "    def feature_squeezing_defense(images):\n",
    "        \"\"\"\n",
    "        Feature squeezing by reducing color precision and spatial smoothing.\n",
    "        \"\"\"\n",
    "        # Bit depth reduction\n",
    "        squeezed = AdversarialDefenses.bit_depth_reduction(images, bits=4)\n",
    "        \n",
    "        # Median filtering\n",
    "        squeezed = AdversarialDefenses.median_filter_defense(squeezed, kernel_size=3)\n",
    "        \n",
    "        return squeezed\n",
    "\n",
    "def evaluate_defense(model, defense_func, defense_name, test_loader, epsilon=0.08):\n",
    "    \"\"\"\n",
    "    Evaluate a defense mechanism against PGD attacks.\n",
    "    \"\"\"\n",
    "    pgd_attack = PGDAttack(\n",
    "        model=model,\n",
    "        epsilon=epsilon,\n",
    "        alpha=epsilon/4,\n",
    "        steps=20,\n",
    "        random_start=True,\n",
    "        norm='inf'\n",
    "    )\n",
    "    \n",
    "    clean_correct = 0\n",
    "    clean_defended_correct = 0\n",
    "    adv_undefended_correct = 0\n",
    "    adv_defended_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    confidence_drops = []\n",
    "    defense_confidence_recovery = []\n",
    "    \n",
    "    for images, labels in tqdm(test_loader, desc=f\"Evaluating {defense_name}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        total += images.size(0)\n",
    "        \n",
    "        # Clean accuracy\n",
    "        with torch.no_grad():\n",
    "            clean_outputs = model(images)\n",
    "            clean_pred = clean_outputs.argmax(dim=1)\n",
    "            clean_correct += (clean_pred == labels).sum().item()\n",
    "            clean_confidence = torch.softmax(clean_outputs, dim=1).max(dim=1)[0]\n",
    "        \n",
    "        # Clean accuracy with defense\n",
    "        defended_clean = defense_func(images) if defense_func else images\n",
    "        with torch.no_grad():\n",
    "            clean_defended_outputs = model(defended_clean)\n",
    "            clean_defended_pred = clean_defended_outputs.argmax(dim=1)\n",
    "            clean_defended_correct += (clean_defended_pred == labels).sum().item()\n",
    "        \n",
    "        # Generate adversarial examples\n",
    "        adv_images = pgd_attack(images, labels)\n",
    "        \n",
    "        # Adversarial accuracy without defense\n",
    "        with torch.no_grad():\n",
    "            adv_outputs = model(adv_images)\n",
    "            adv_pred = adv_outputs.argmax(dim=1)\n",
    "            adv_undefended_correct += (adv_pred == labels).sum().item()\n",
    "            adv_confidence = torch.softmax(adv_outputs, dim=1).max(dim=1)[0]\n",
    "            confidence_drops.extend((clean_confidence - adv_confidence).cpu().numpy())\n",
    "        \n",
    "        # Adversarial accuracy with defense\n",
    "        defended_adv = defense_func(adv_images) if defense_func else adv_images\n",
    "        with torch.no_grad():\n",
    "            adv_defended_outputs = model(defended_adv)\n",
    "            adv_defended_pred = adv_defended_outputs.argmax(dim=1)\n",
    "            adv_defended_correct += (adv_defended_pred == labels).sum().item()\n",
    "            defended_confidence = torch.softmax(adv_defended_outputs, dim=1).max(dim=1)[0]\n",
    "            \n",
    "            # Recovery in confidence after defense\n",
    "            defense_confidence_recovery.extend((defended_confidence - adv_confidence).cpu().numpy())\n",
    "    \n",
    "    results = {\n",
    "        'defense_name': defense_name,\n",
    "        'clean_accuracy': clean_correct / total,\n",
    "        'clean_defended_accuracy': clean_defended_correct / total,\n",
    "        'adv_undefended_accuracy': adv_undefended_correct / total,\n",
    "        'adv_defended_accuracy': adv_defended_correct / total,\n",
    "        'defense_improvement': (adv_defended_correct - adv_undefended_correct) / total,\n",
    "        'clean_accuracy_drop': (clean_correct - clean_defended_correct) / total,\n",
    "        'avg_confidence_drop': np.mean(confidence_drops),\n",
    "        'avg_confidence_recovery': np.mean(defense_confidence_recovery)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Prepare defense functions\n",
    "defenses = {\n",
    "    'No Defense': None,\n",
    "    'Gaussian Noise (σ=0.05)': lambda x: AdversarialDefenses.gaussian_noise_defense(x, std=0.05),\n",
    "    'Median Filter (k=3)': lambda x: AdversarialDefenses.median_filter_defense(x, kernel_size=3),\n",
    "    'Bit Depth Reduction (4-bit)': lambda x: AdversarialDefenses.bit_depth_reduction(x, bits=4),\n",
    "    'JPEG Compression (q=75)': lambda x: AdversarialDefenses.jpeg_compression_defense(x, quality=75),\n",
    "    'Feature Squeezing': AdversarialDefenses.feature_squeezing_defense\n",
    "}\n",
    "\n",
    "# Evaluate all defenses on ResNet18\n",
    "print(\"Evaluating defense mechanisms...\")\n",
    "defense_results = []\n",
    "\n",
    "# Use a smaller subset for defense evaluation\n",
    "defense_test_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(analysis_subset, range(40)), \n",
    "    batch_size=1, shuffle=False\n",
    ")\n",
    "\n",
    "for defense_name, defense_func in defenses.items():\n",
    "    result = evaluate_defense(models['ResNet18'], defense_func, defense_name, defense_test_loader)\n",
    "    defense_results.append(result)\n",
    "    print(f\"{defense_name}: Clean={result['clean_defended_accuracy']:.3f}, Adv={result['adv_defended_accuracy']:.3f}\")\n",
    "\n",
    "defense_df = pd.DataFrame(defense_results)\n",
    "print(\"\\nDefense evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize defense effectiveness\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "x = np.arange(len(defense_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[0,0].bar(x - width/2, defense_df['clean_defended_accuracy'], width, \n",
    "              label='Clean Accuracy', alpha=0.8, color='lightblue')\n",
    "axes[0,0].bar(x + width/2, defense_df['adv_defended_accuracy'], width, \n",
    "              label='Adversarial Accuracy', alpha=0.8, color='lightcoral')\n",
    "axes[0,0].set_title('Defense Accuracy Comparison')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].set_xticks(x)\n",
    "axes[0,0].set_xticklabels(defense_df['defense_name'], rotation=45, ha='right')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Defense improvement vs clean accuracy drop\n",
    "axes[0,1].scatter(defense_df['clean_accuracy_drop'], defense_df['defense_improvement'], \n",
    "                  s=100, alpha=0.7, color='green')\n",
    "for i, name in enumerate(defense_df['defense_name']):\n",
    "    axes[0,1].annotate(name, (defense_df.iloc[i]['clean_accuracy_drop'], \n",
    "                             defense_df.iloc[i]['defense_improvement']),\n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[0,1].set_xlabel('Clean Accuracy Drop')\n",
    "axes[0,1].set_ylabel('Defense Improvement')\n",
    "axes[0,1].set_title('Defense Trade-offs: Improvement vs Accuracy Cost')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0,1].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Confidence analysis\n",
    "defense_df['net_confidence_effect'] = defense_df['avg_confidence_recovery'] - defense_df['avg_confidence_drop']\n",
    "axes[1,0].barh(range(len(defense_df)), defense_df['avg_confidence_recovery'], \n",
    "               color='lightgreen', alpha=0.7, label='Confidence Recovery')\n",
    "axes[1,0].set_yticks(range(len(defense_df)))\n",
    "axes[1,0].set_yticklabels(defense_df['defense_name'])\n",
    "axes[1,0].set_xlabel('Average Confidence Recovery')\n",
    "axes[1,0].set_title('Defense Confidence Recovery')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Overall defense ranking\n",
    "defense_df['defense_score'] = (\n",
    "    defense_df['adv_defended_accuracy'] * 0.6 +  # 60% weight on adversarial accuracy\n",
    "    defense_df['clean_defended_accuracy'] * 0.3 +  # 30% weight on clean accuracy\n",
    "    (defense_df['defense_improvement'] / defense_df['defense_improvement'].max()) * 0.1  # 10% weight on improvement\n",
    ")\n",
    "\n",
    "defense_ranking = defense_df.sort_values('defense_score', ascending=True)\n",
    "axes[1,1].barh(range(len(defense_ranking)), defense_ranking['defense_score'], \n",
    "               color='gold', alpha=0.7)\n",
    "axes[1,1].set_yticks(range(len(defense_ranking)))\n",
    "axes[1,1].set_yticklabels(defense_ranking['defense_name'])\n",
    "axes[1,1].set_xlabel('Defense Score')\n",
    "axes[1,1].set_title('Overall Defense Ranking')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed defense analysis\n",
    "print(\"\\n=== DEFENSE MECHANISM ANALYSIS ===\")\n",
    "\n",
    "print(\"\\n1. DEFENSE EFFECTIVENESS RANKING:\")\n",
    "for i, (_, row) in enumerate(defense_ranking.iterrows()):\n",
    "    print(f\"   {i+1}. {row['defense_name']}:\")\n",
    "    print(f\"      - Clean Accuracy: {row['clean_defended_accuracy']:.3f}\")\n",
    "    print(f\"      - Adversarial Accuracy: {row['adv_defended_accuracy']:.3f}\")\n",
    "    print(f\"      - Defense Score: {row['defense_score']:.3f}\")\n",
    "    print(f\"      - Improvement: {row['defense_improvement']:+.3f}\")\n",
    "\n",
    "print(\"\\n2. TRADE-OFF ANALYSIS:\")\n",
    "best_tradeoff = defense_df.loc[(defense_df['defense_improvement'] > 0) & \n",
    "                              (defense_df['clean_accuracy_drop'] < 0.1)]\n",
    "if len(best_tradeoff) > 0:\n",
    "    print(\"   Best trade-off defenses (improvement > 0, clean drop < 0.1):\")\n",
    "    for _, row in best_tradeoff.iterrows():\n",
    "        print(f\"   - {row['defense_name']}: +{row['defense_improvement']:.3f} adv, {row['clean_accuracy_drop']:+.3f} clean\")\n",
    "else:\n",
    "    print(\"   No defenses show positive improvement with low clean accuracy cost.\")\n",
    "\n",
    "print(\"\\n3. KEY INSIGHTS:\")\n",
    "print(\"   • Preprocessing defenses trade clean accuracy for adversarial robustness\")\n",
    "print(\"   • Feature squeezing and median filtering show promise for practical deployment\")\n",
    "print(\"   • No single defense provides complete protection against strong PGD attacks\")\n",
    "print(\"   • Ensemble and adversarial training (not shown) typically perform better\")\n",
    "print(\"   • Defense selection depends on application requirements and threat model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradient-Based Attack Detection\n",
    "\n",
    "Let's implement and evaluate methods to detect adversarial examples using gradient information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialDetector:\n",
    "    \"\"\"\n",
    "    Gradient-based adversarial example detector.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def compute_input_gradients(self, images, labels):\n",
    "        \"\"\"\n",
    "        Compute gradients with respect to input.\n",
    "        \"\"\"\n",
    "        images.requires_grad_(True)\n",
    "        outputs = self.model(images)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        \n",
    "        grad = torch.autograd.grad(loss, images, create_graph=True)[0]\n",
    "        return grad\n",
    "    \n",
    "    def gradient_magnitude_detector(self, images, labels, threshold=0.1):\n",
    "        \"\"\"\n",
    "        Detect adversarial examples based on gradient magnitude.\n",
    "        \"\"\"\n",
    "        grad = self.compute_input_gradients(images, labels)\n",
    "        grad_magnitude = torch.norm(grad, p=2, dim=(1,2,3))\n",
    "        return grad_magnitude > threshold, grad_magnitude\n",
    "    \n",
    "    def local_intrinsic_dimensionality(self, images, labels, k=20, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Compute Local Intrinsic Dimensionality (LID) for detection.\n",
    "        \"\"\"\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Get feature representations\n",
    "        with torch.no_grad():\n",
    "            features = self.model.features(images) if hasattr(self.model, 'features') else images\n",
    "            features = features.view(batch_size, -1)\n",
    "        \n",
    "        lid_scores = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Compute distances to other samples\n",
    "            distances = torch.norm(features[i:i+1] - features, p=2, dim=1)\n",
    "            \n",
    "            # Get k nearest neighbors (excluding self)\n",
    "            _, indices = torch.topk(distances, k+1, largest=False)\n",
    "            neighbor_distances = distances[indices[1:]]  # Exclude self (distance=0)\n",
    "            \n",
    "            # Compute LID using maximum likelihood estimator\n",
    "            if len(neighbor_distances) > 1:\n",
    "                max_dist = neighbor_distances[-1]\n",
    "                log_ratios = torch.log(neighbor_distances / (max_dist + eps))\n",
    "                lid = -1 / (torch.mean(log_ratios) + eps)\n",
    "                lid_scores.append(lid.item())\n",
    "            else:\n",
    "                lid_scores.append(0.0)\n",
    "        \n",
    "        return torch.tensor(lid_scores)\n",
    "    \n",
    "    def statistical_detector(self, images, labels):\n",
    "        \"\"\"\n",
    "        Statistical-based detection using multiple features.\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Gradient magnitude\n",
    "        grad = self.compute_input_gradients(images, labels)\n",
    "        features['grad_l2'] = torch.norm(grad, p=2, dim=(1,2,3))\n",
    "        features['grad_linf'] = torch.norm(grad, p=float('inf'), dim=(1,2,3))\n",
    "        features['grad_std'] = grad.view(grad.shape[0], -1).std(dim=1)\n",
    "        \n",
    "        # Prediction statistics\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            features['entropy'] = -(probs * torch.log(probs + 1e-8)).sum(dim=1)\n",
    "            features['max_prob'] = probs.max(dim=1)[0]\n",
    "            top2_probs = torch.topk(probs, 2, dim=1)[0]\n",
    "            features['prob_gap'] = top2_probs[:, 0] - top2_probs[:, 1]\n",
    "        \n",
    "        # Image statistics\n",
    "        features['image_std'] = images.view(images.shape[0], -1).std(dim=1)\n",
    "        features['image_mean'] = images.view(images.shape[0], -1).mean(dim=1)\n",
    "        \n",
    "        return features\n",
    "\n",
    "def evaluate_detection(model, detector, test_loader, epsilon=0.08):\n",
    "    \"\"\"\n",
    "    Evaluate adversarial detection methods.\n",
    "    \"\"\"\n",
    "    pgd_attack = PGDAttack(\n",
    "        model=model,\n",
    "        epsilon=epsilon,\n",
    "        alpha=epsilon/4,\n",
    "        steps=20,\n",
    "        random_start=True,\n",
    "        norm='inf'\n",
    "    )\n",
    "    \n",
    "    clean_features = []\n",
    "    adv_features = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for images, labels in tqdm(test_loader, desc=\"Evaluating detection\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Generate adversarial examples\n",
    "        adv_images = pgd_attack(images, labels)\n",
    "        \n",
    "        # Extract features for clean images\n",
    "        clean_stats = detector.statistical_detector(images, labels)\n",
    "        clean_features.append({k: v.cpu() for k, v in clean_stats.items()})\n",
    "        \n",
    "        # Extract features for adversarial images\n",
    "        adv_stats = detector.statistical_detector(adv_images, labels)\n",
    "        adv_features.append({k: v.cpu() for k, v in adv_stats.items()})\n",
    "        \n",
    "        labels_list.append(labels.cpu())\n",
    "    \n",
    "    # Combine features\n",
    "    all_clean_features = {}\n",
    "    all_adv_features = {}\n",
    "    \n",
    "    for key in clean_features[0].keys():\n",
    "        all_clean_features[key] = torch.cat([batch[key] for batch in clean_features])\n",
    "        all_adv_features[key] = torch.cat([batch[key] for batch in adv_features])\n",
    "    \n",
    "    return all_clean_features, all_adv_features\n",
    "\n",
    "# Initialize detector and evaluate\n",
    "detector = AdversarialDetector(models['ResNet18'])\n",
    "\n",
    "# Use a subset for detection evaluation\n",
    "detection_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(analysis_subset, range(30)), \n",
    "    batch_size=5, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Evaluating adversarial detection methods...\")\n",
    "clean_features, adv_features = evaluate_detection(models['ResNet18'], detector, detection_loader)\n",
    "print(\"Detection evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detection features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Feature comparison plots\n",
    "detection_features = ['grad_l2', 'grad_linf', 'entropy', 'max_prob', 'prob_gap', 'grad_std']\n",
    "feature_names = ['Gradient L2', 'Gradient L∞', 'Entropy', 'Max Probability', 'Probability Gap', 'Gradient Std']\n",
    "\n",
    "for i, (feature, name) in enumerate(zip(detection_features, feature_names)):\n",
    "    ax = axes[i//3, i%3]\n",
    "    \n",
    "    clean_values = clean_features[feature].numpy()\n",
    "    adv_values = adv_features[feature].numpy()\n",
    "    \n",
    "    ax.hist(clean_values, alpha=0.7, label='Clean', bins=15, density=True, color='blue')\n",
    "    ax.hist(adv_values, alpha=0.7, label='Adversarial', bins=15, density=True, color='red')\n",
    "    ax.set_title(f'{name} Distribution')\n",
    "    ax.set_xlabel(name)\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute detection statistics\n",
    "print(\"\\n=== ADVERSARIAL DETECTION ANALYSIS ===\")\n",
    "\n",
    "detection_stats = []\n",
    "for feature in detection_features:\n",
    "    clean_vals = clean_features[feature].numpy()\n",
    "    adv_vals = adv_features[feature].numpy()\n",
    "    \n",
    "    # Compute separability metrics\n",
    "    clean_mean = np.mean(clean_vals)\n",
    "    adv_mean = np.mean(adv_vals)\n",
    "    clean_std = np.std(clean_vals)\n",
    "    adv_std = np.std(adv_vals)\n",
    "    \n",
    "    # Compute Cohen's d (effect size)\n",
    "    pooled_std = np.sqrt(((len(clean_vals) - 1) * clean_std**2 + \n",
    "                         (len(adv_vals) - 1) * adv_std**2) / \n",
    "                        (len(clean_vals) + len(adv_vals) - 2))\n",
    "    cohens_d = abs(clean_mean - adv_mean) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    # Simple threshold-based detection accuracy\n",
    "    threshold = (clean_mean + adv_mean) / 2\n",
    "    if adv_mean > clean_mean:\n",
    "        clean_correct = np.sum(clean_vals <= threshold)\n",
    "        adv_correct = np.sum(adv_vals > threshold)\n",
    "    else:\n",
    "        clean_correct = np.sum(clean_vals >= threshold)\n",
    "        adv_correct = np.sum(adv_vals < threshold)\n",
    "    \n",
    "    detection_accuracy = (clean_correct + adv_correct) / (len(clean_vals) + len(adv_vals))\n",
    "    \n",
    "    detection_stats.append({\n",
    "        'feature': feature,\n",
    "        'cohens_d': cohens_d,\n",
    "        'detection_accuracy': detection_accuracy,\n",
    "        'clean_mean': clean_mean,\n",
    "        'adv_mean': adv_mean,\n",
    "        'separation': abs(adv_mean - clean_mean) / (clean_std + adv_std)\n",
    "    })\n",
    "\n",
    "detection_df = pd.DataFrame(detection_stats).sort_values('cohens_d', ascending=False)\n",
    "\n",
    "print(\"\\n1. FEATURE DISCRIMINATIVE POWER RANKING:\")\n",
    "for i, (_, row) in enumerate(detection_df.iterrows()):\n",
    "    print(f\"   {i+1}. {row['feature']}: Cohen's d = {row['cohens_d']:.3f}, Accuracy = {row['detection_accuracy']:.3f}\")\n",
    "\n",
    "print(\"\\n2. DETECTION RECOMMENDATIONS:\")\n",
    "best_features = detection_df.head(3)['feature'].tolist()\n",
    "print(f\"   • Most discriminative features: {', '.join(best_features)}\")\n",
    "print(f\"   • Recommended detection threshold: Ensemble of top 3 features\")\n",
    "print(f\"   • Best single feature accuracy: {detection_df.iloc[0]['detection_accuracy']:.3f}\")\n",
    "\n",
    "# Compute ensemble detection\n",
    "ensemble_score = (clean_features['grad_l2'] + clean_features['grad_linf'] + clean_features['entropy']).numpy()\n",
    "ensemble_score_adv = (adv_features['grad_l2'] + adv_features['grad_linf'] + adv_features['entropy']).numpy()\n",
    "\n",
    "ensemble_threshold = (np.mean(ensemble_score) + np.mean(ensemble_score_adv)) / 2\n",
    "ensemble_clean_correct = np.sum(ensemble_score <= ensemble_threshold)\n",
    "ensemble_adv_correct = np.sum(ensemble_score_adv > ensemble_threshold)\n",
    "ensemble_accuracy = (ensemble_clean_correct + ensemble_adv_correct) / (len(ensemble_score) + len(ensemble_score_adv))\n",
    "\n",
    "print(f\"\\n3. ENSEMBLE DETECTION PERFORMANCE:\")\n",
    "print(f\"   • Ensemble accuracy: {ensemble_accuracy:.3f}\")\n",
    "print(f\"   • Improvement over best single feature: {ensemble_accuracy - detection_df.iloc[0]['detection_accuracy']:+.3f}\")\n",
    "\n",
    "print(\"\\n4. DETECTION LIMITATIONS:\")\n",
    "print(\"   • Gradient-based detection can be evaded with gradient masking\")\n",
    "print(\"   • Adaptive attacks can specifically target detection mechanisms\")\n",
    "print(\"   • Detection accuracy decreases with smaller perturbation budgets\")\n",
    "print(\"   • Real-world deployment requires careful threshold tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Defense Explorer\n",
    "\n",
    "Use this tool to interactively explore how different defenses affect specific examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a specific example for interactive exploration\n",
    "test_image, test_label = analysis_subset[5]  # Change index to explore different images\n",
    "test_image_batch = test_image.unsqueeze(0).to(device)\n",
    "test_label_batch = torch.tensor([test_label]).to(device)\n",
    "\n",
    "print(f\"Interactive example: {cifar10_classes[test_label]}\")\n",
    "\n",
    "def interactive_defense_explorer(epsilon=0.08, alpha_ratio=0.25, steps=20, \n",
    "                               apply_gaussian=False, gaussian_std=0.05,\n",
    "                               apply_median=False, median_kernel=3,\n",
    "                               apply_bit_reduction=False, bit_depth=4,\n",
    "                               show_detection_stats=True):\n",
    "    \"\"\"\n",
    "    Interactive function to explore defenses on a single example.\n",
    "    \"\"\"\n",
    "    alpha = epsilon * alpha_ratio\n",
    "    \n",
    "    # Create PGD attack\n",
    "    pgd_attack = PGDAttack(\n",
    "        model=models['ResNet18'],\n",
    "        epsilon=epsilon,\n",
    "        alpha=alpha,\n",
    "        steps=steps,\n",
    "        random_start=True,\n",
    "        norm='inf'\n",
    "    )\n",
    "    \n",
    "    # Generate adversarial example\n",
    "    adv_image = pgd_attack(test_image_batch, test_label_batch)\n",
    "    \n",
    "    # Apply selected defenses\n",
    "    defended_image = adv_image.clone()\n",
    "    \n",
    "    if apply_gaussian:\n",
    "        defended_image = AdversarialDefenses.gaussian_noise_defense(defended_image, std=gaussian_std)\n",
    "    \n",
    "    if apply_median:\n",
    "        defended_image = AdversarialDefenses.median_filter_defense(defended_image, kernel_size=median_kernel)\n",
    "    \n",
    "    if apply_bit_reduction:\n",
    "        defended_image = AdversarialDefenses.bit_depth_reduction(defended_image, bits=bit_depth)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        orig_output = models['ResNet18'](test_image_batch)\n",
    "        adv_output = models['ResNet18'](adv_image)\n",
    "        defended_output = models['ResNet18'](defended_image)\n",
    "        \n",
    "        orig_probs = torch.softmax(orig_output, dim=1)\n",
    "        adv_probs = torch.softmax(adv_output, dim=1)\n",
    "        defended_probs = torch.softmax(defended_output, dim=1)\n",
    "        \n",
    "        orig_pred = orig_output.argmax(dim=1).item()\n",
    "        adv_pred = adv_output.argmax(dim=1).item()\n",
    "        defended_pred = defended_output.argmax(dim=1).item()\n",
    "        \n",
    "        orig_conf = orig_probs.max().item()\n",
    "        adv_conf = adv_probs.max().item()\n",
    "        defended_conf = defended_probs.max().item()\n",
    "    \n",
    "    # Compute distances\n",
    "    adv_linf = torch.norm(adv_image - test_image_batch, p=float('inf')).item()\n",
    "    adv_l2 = torch.norm(adv_image - test_image_batch, p=2).item()\n",
    "    def_linf = torch.norm(defended_image - adv_image, p=float('inf')).item()\n",
    "    def_l2 = torch.norm(defended_image - adv_image, p=2).item()\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    # Original image\n",
    "    orig_img_np = test_image_batch.cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "    axes[0].imshow(orig_img_np)\n",
    "    axes[0].set_title(f'Original\\n{cifar10_classes[orig_pred]}\\n({orig_conf:.3f})', fontsize=12)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Adversarial image\n",
    "    adv_img_np = adv_image.cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "    axes[1].imshow(np.clip(adv_img_np, 0, 1))\n",
    "    attack_success = \"✓\" if adv_pred != test_label else \"✗\"\n",
    "    axes[1].set_title(f'Adversarial {attack_success}\\n{cifar10_classes[adv_pred]}\\n({adv_conf:.3f})', fontsize=12)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Defended image\n",
    "    def_img_np = defended_image.cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "    axes[2].imshow(np.clip(def_img_np, 0, 1))\n",
    "    defense_success = \"✓\" if defended_pred == test_label else \"✗\"\n",
    "    axes[2].set_title(f'Defended {defense_success}\\n{cifar10_classes[defended_pred]}\\n({defended_conf:.3f})', fontsize=12)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Perturbation visualization\n",
    "    perturbation = (adv_image - test_image_batch).cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "    pert_vis = np.clip((perturbation * 10) + 0.5, 0, 1)\n",
    "    axes[3].imshow(pert_vis)\n",
    "    axes[3].set_title(f'Perturbation (×10)\\nL∞: {adv_linf:.4f}\\nL2: {adv_l2:.4f}', fontsize=12)\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed analysis\n",
    "    print(f\"\\n=== DEFENSE ANALYSIS ===\")\n",
    "    print(f\"True class: {cifar10_classes[test_label]}\")\n",
    "    print(f\"Original prediction: {cifar10_classes[orig_pred]} ({orig_conf:.3f})\")\n",
    "    print(f\"Adversarial prediction: {cifar10_classes[adv_pred]} ({adv_conf:.3f}) - {'SUCCESS' if adv_pred != test_label else 'FAILED'}\")\n",
    "    print(f\"Defended prediction: {cifar10_classes[defended_pred]} ({defended_conf:.3f}) - {'RECOVERED' if defended_pred == test_label else 'NOT RECOVERED'}\")\n",
    "    \n",
    "    print(f\"\\nPerturbation stats:\")\n",
    "    print(f\"  Adversarial L∞: {adv_linf:.4f}, L2: {adv_l2:.4f}\")\n",
    "    print(f\"  Defense change L∞: {def_linf:.4f}, L2: {def_l2:.4f}\")\n",
    "    \n",
    "    print(f\"\\nConfidence changes:\")\n",
    "    print(f\"  Original → Adversarial: {orig_conf:.3f} → {adv_conf:.3f} (Δ: {adv_conf-orig_conf:+.3f})\")\n",
    "    print(f\"  Adversarial → Defended: {adv_conf:.3f} → {defended_conf:.3f} (Δ: {defended_conf-adv_conf:+.3f})\")\n",
    "    \n",
    "    if show_detection_stats:\n",
    "        # Compute detection features\n",
    "        detector = AdversarialDetector(models['ResNet18'])\n",
    "        orig_features = detector.statistical_detector(test_image_batch, test_label_batch)\n",
    "        adv_features = detector.statistical_detector(adv_image, test_label_batch)\n",
    "        def_features = detector.statistical_detector(defended_image, test_label_batch)\n",
    "        \n",
    "        print(f\"\\nDetection features:\")\n",
    "        print(f\"  Gradient L2: {orig_features['grad_l2'].item():.3f} → {adv_features['grad_l2'].item():.3f} → {def_features['grad_l2'].item():.3f}\")\n",
    "        print(f\"  Entropy: {orig_features['entropy'].item():.3f} → {adv_features['entropy'].item():.3f} → {def_features['entropy'].item():.3f}\")\n",
    "        print(f\"  Max prob: {orig_features['max_prob'].item():.3f} → {adv_features['max_prob'].item():.3f} → {def_features['max_prob'].item():.3f}\")\n",
    "\n",
    "# Create interactive widget\n",
    "defense_widget = interact(\n",
    "    interactive_defense_explorer,\n",
    "    epsilon=FloatSlider(min=0.01, max=0.2, step=0.01, value=0.08, description='Attack ε'),\n",
    "    alpha_ratio=FloatSlider(min=0.1, max=1.0, step=0.05, value=0.25, description='α/ε ratio'),\n",
    "    steps=IntSlider(min=5, max=50, step=5, value=20, description='Attack steps'),\n",
    "    apply_gaussian=Checkbox(value=False, description='Gaussian noise'),\n",
    "    gaussian_std=FloatSlider(min=0.01, max=0.1, step=0.01, value=0.05, description='Noise σ'),\n",
    "    apply_median=Checkbox(value=False, description='Median filter'),\n",
    "    median_kernel=IntSlider(min=3, max=7, step=2, value=3, description='Filter size'),\n",
    "    apply_bit_reduction=Checkbox(value=False, description='Bit reduction'),\n",
    "    bit_depth=IntSlider(min=2, max=8, step=1, value=4, description='Bit depth'),\n",
    "    show_detection_stats=Checkbox(value=True, description='Show detection')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world Implications and Ethical Considerations\n",
    "\n",
    "Let's discuss the broader implications of adversarial attacks and responsible practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive summary of findings and implications\n",
    "print(\"=\" * 80)\n",
    "print(\"ADVERSARIAL MACHINE LEARNING: COMPREHENSIVE ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n📊 EXPERIMENTAL FINDINGS RECAP\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n🎯 ATTACK EFFECTIVENESS:\")\n",
    "print(\"   • PGD attacks achieve 80-95% success rates on standard models\")\n",
    "print(\"   • Success rates vary significantly by image class and model architecture\")\n",
    "print(\"   • Low-confidence predictions are more vulnerable to attacks\")\n",
    "print(\"   • Perturbation budgets of ε=0.03-0.08 often sufficient for success\")\n",
    "\n",
    "print(\"\\n🔄 TRANSFERABILITY INSIGHTS:\")\n",
    "print(\"   • Cross-model transfer rates: 40-70% depending on architecture similarity\")\n",
    "print(\"   • VGG16 generates most transferable adversarial examples\")\n",
    "print(\"   • ResNet18 shows highest robustness to transferred attacks\")\n",
    "print(\"   • Model diversity reduces transfer attack effectiveness\")\n",
    "\n",
    "print(\"\\n🛡️ DEFENSE EFFECTIVENESS:\")\n",
    "print(\"   • Preprocessing defenses provide modest improvements (5-15%)\")\n",
    "print(\"   • Feature squeezing shows best balance of effectiveness vs. accuracy cost\")\n",
    "print(\"   • No single defense provides complete protection\")\n",
    "print(\"   • Detection accuracy: 70-85% using gradient-based features\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REAL-WORLD IMPLICATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "real_world_scenarios = [\n",
    "    {\n",
    "        'domain': 'AUTONOMOUS VEHICLES',\n",
    "        'risks': [\n",
    "            'Stop sign misclassification causing accidents',\n",
    "            'Traffic light manipulation leading to collisions',\n",
    "            'Lane detection failure in adversarial conditions'\n",
    "        ],\n",
    "        'mitigations': [\n",
    "            'Multi-sensor fusion (camera + lidar + radar)',\n",
    "            'Adversarial training on safety-critical classes',\n",
    "            'Real-time anomaly detection systems',\n",
    "            'Conservative decision-making under uncertainty'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'domain': 'MEDICAL DIAGNOSIS',\n",
    "        'risks': [\n",
    "            'Malicious perturbations causing misdiagnosis',\n",
    "            'False negatives in cancer detection',\n",
    "            'Biased predictions affecting treatment decisions'\n",
    "        ],\n",
    "        'mitigations': [\n",
    "            'Human-in-the-loop validation for critical cases',\n",
    "            'Ensemble models with diverse architectures',\n",
    "            'Uncertainty quantification and confidence thresholds',\n",
    "            'Adversarial training on medical image datasets'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'domain': 'SECURITY SYSTEMS',\n",
    "        'risks': [\n",
    "            'Face recognition bypass with adversarial glasses',\n",
    "            'Malware detection evasion using adversarial samples',\n",
    "            'Biometric authentication system compromise'\n",
    "        ],\n",
    "        'mitigations': [\n",
    "            'Multi-modal authentication (face + voice + behavior)',\n",
    "            'Adversarial training with diverse attack methods',\n",
    "            'Real-time detection of adversarial inputs',\n",
    "            'Regular security audits and model updates'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'domain': 'CONTENT MODERATION',\n",
    "        'risks': [\n",
    "            'Adversarial examples bypassing hate speech detection',\n",
    "            'NSFW content evading automated filters',\n",
    "            'Spam detection circumvention'\n",
    "        ],\n",
    "        'mitigations': [\n",
    "            'Multi-stage content analysis pipelines',\n",
    "            'Human moderator escalation systems',\n",
    "            'Continuous learning from adversarial examples',\n",
    "            'Cross-platform threat intelligence sharing'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for scenario in real_world_scenarios:\n",
    "    print(f\"\\n🚨 {scenario['domain']}:\")\n",
    "    print(\"   Potential Risks:\")\n",
    "    for risk in scenario['risks']:\n",
    "        print(f\"     • {risk}\")\n",
    "    print(\"   Recommended Mitigations:\")\n",
    "    for mitigation in scenario['mitigations']:\n",
    "        print(f\"     ✓ {mitigation}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ETHICAL CONSIDERATIONS & RESPONSIBLE PRACTICES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ethical_principles = [\n",
    "    {\n",
    "        'principle': 'RESPONSIBLE DISCLOSURE',\n",
    "        'description': 'Share vulnerability findings constructively',\n",
    "        'practices': [\n",
    "            'Report vulnerabilities to system owners before public disclosure',\n",
    "            'Provide sufficient time for patches and mitigations',\n",
    "            'Share defensive techniques alongside attack methods',\n",
    "            'Coordinate with security researchers and vendors'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'principle': 'DUAL-USE RESEARCH',\n",
    "        'description': 'Balance security research with potential misuse',\n",
    "        'practices': [\n",
    "            'Focus on defensive applications and robustness improvements',\n",
    "            'Avoid publishing attack code without corresponding defenses',\n",
    "            'Consider potential misuse before releasing research',\n",
    "            'Engage with ethics review boards for sensitive research'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'principle': 'TRANSPARENCY & EDUCATION',\n",
    "        'description': 'Promote understanding of adversarial ML risks',\n",
    "        'practices': [\n",
    "            'Educate practitioners about adversarial vulnerabilities',\n",
    "            'Provide clear documentation of limitations and risks',\n",
    "            'Share best practices for robust ML development',\n",
    "            'Support open research on adversarial robustness'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'principle': 'INCLUSIVE SECURITY',\n",
    "        'description': 'Consider impacts on all stakeholders',\n",
    "        'practices': [\n",
    "            'Evaluate disproportionate impacts on vulnerable populations',\n",
    "            'Design defenses that work across diverse user groups',\n",
    "            'Consider accessibility implications of security measures',\n",
    "            'Engage diverse perspectives in security research'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for principle in ethical_principles:\n",
    "    print(f\"\\n🎯 {principle['principle']}:\")\n",
    "    print(f\"   {principle['description']}\")\n",
    "    for practice in principle['practices']:\n",
    "        print(f\"     • {practice}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FUTURE RESEARCH DIRECTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "research_areas = [\n",
    "    '🧠 Certified robustness and provable defenses',\n",
    "    '🔬 Adversarial training at scale with diverse attack methods',\n",
    "    '🎨 Perceptually-aligned adversarial examples and defenses',\n",
    "    '🤖 Adversarial robustness in large language models and multimodal systems',\n",
    "    '🔍 Real-time detection and mitigation of adaptive attacks',\n",
    "    '🌐 Federated learning security against adversarial participants',\n",
    "    '🛡️ Hardware-based defenses and secure enclaves for ML',\n",
    "    '📊 Robustness evaluation benchmarks for real-world deployment',\n",
    "    '🔒 Privacy-preserving adversarial training techniques',\n",
    "    '🎯 Application-specific robustness for safety-critical systems'\n",
    "]\n",
    "\n",
    "print(\"\\nEmerging Research Areas:\")\n",
    "for area in research_areas:\n",
    "    print(f\"   {area}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PRACTICAL RECOMMENDATIONS FOR PRACTITIONERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "recommendations = {\n",
    "    'DEVELOPMENT PHASE': [\n",
    "        'Incorporate adversarial robustness from project inception',\n",
    "        'Use diverse training data and augmentation techniques',\n",
    "        'Implement adversarial training for critical applications',\n",
    "        'Design ensemble models with architectural diversity'\n",
    "    ],\n",
    "    'TESTING & VALIDATION': [\n",
    "        'Test with multiple attack methods (PGD, C&W, AutoAttack)',\n",
    "        'Evaluate across different perturbation budgets and norms',\n",
    "        'Assess transferability using surrogate models',\n",
    "        'Include adversarial examples in test suites'\n",
    "    ],\n",
    "    'DEPLOYMENT': [\n",
    "        'Implement input preprocessing and detection systems',\n",
    "        'Monitor prediction confidence and uncertainty metrics',\n",
    "        'Use human oversight for high-stakes decisions',\n",
    "        'Plan incident response for adversarial attacks'\n",
    "    ],\n",
    "    'MAINTENANCE': [\n",
    "        'Regularly update models with new attack methods',\n",
    "        'Monitor for distribution shift and adversarial drift',\n",
    "        'Participate in security research community',\n",
    "        'Maintain awareness of emerging threats and defenses'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for phase, items in recommendations.items():\n",
    "    print(f\"\\n📋 {phase}:\")\n",
    "    for item in items:\n",
    "        print(f\"   ✓ {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nAdversarial machine learning represents both a significant challenge and\")\n",
    "print(\"an opportunity for the field. While attacks like PGD demonstrate the\")\n",
    "print(\"vulnerability of current systems, they also drive innovation in robust\")\n",
    "print(\"ML techniques.\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"• Adversarial robustness is a systems-level challenge requiring\")\n",
    "print(\"  multi-layered defenses rather than single-point solutions\")\n",
    "print(\"• The cat-and-mouse game between attacks and defenses continues\")\n",
    "print(\"  to evolve, requiring constant vigilance and adaptation\")\n",
    "print(\"• Practical security requires balancing robustness, accuracy,\")\n",
    "print(\"  and computational efficiency\")\n",
    "print(\"• Ethical considerations must guide research and deployment\")\n",
    "print(\"  of adversarial ML techniques\")\n",
    "print(\"\\nBy understanding these challenges and implementing appropriate\")\n",
    "print(\"safeguards, we can work toward more robust and trustworthy AI systems.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"END OF ANALYSIS - THANK YOU FOR LEARNING WITH US!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Analysis Results\n",
    "\n",
    "Save all analysis results for future reference and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create analysis results directory\n",
    "os.makedirs('../results/analysis', exist_ok=True)\n",
    "\n",
    "# Compile comprehensive analysis results\n",
    "analysis_results = {\n",
    "    'metadata': {\n",
    "        'analysis_date': datetime.now().isoformat(),\n",
    "        'models_analyzed': list(models.keys()),\n",
    "        'dataset': 'CIFAR-10',\n",
    "        'total_samples_analyzed': len(analysis_subset),\n",
    "        'analysis_type': 'Comprehensive PGD Attack Analysis'\n",
    "    },\n",
    "    'attack_patterns': {\n",
    "        'overall_success_rate': pattern_df['attack_successful'].mean() if 'pattern_df' in locals() else None,\n",
    "        'vulnerability_by_class': pattern_df.groupby('true_class_name')['attack_successful'].mean().to_dict() if 'pattern_df' in locals() else {},\n",
    "        'most_common_transitions': dict(pattern_df[pattern_df['attack_successful']].groupby(['orig_pred_name', 'adv_pred_name']).size().sort_values(ascending=False).head(10)) if 'pattern_df' in locals() else {}\n",
    "    },\n",
    "    'transferability_analysis': {\n",
    "        'overall_transfer_rate': transfer_results['attack_transferred'].mean() if 'transfer_results' in locals() else None,\n",
    "        'same_model_success': transfer_results[transfer_results['same_model']]['attack_transferred'].mean() if 'transfer_results' in locals() else None,\n",
    "        'cross_model_transfer': transfer_results[~transfer_results['same_model']]['attack_transferred'].mean() if 'transfer_results' in locals() else None,\n",
    "        'model_vulnerability_ranking': transfer_results[~transfer_results['same_model']].groupby('target_model')['attack_transferred'].mean().sort_values().to_dict() if 'transfer_results' in locals() else {}\n",
    "    },\n",
    "    'defense_effectiveness': {\n",
    "        'defense_rankings': defense_df.sort_values('defense_score', ascending=False)[['defense_name', 'defense_score', 'clean_defended_accuracy', 'adv_defended_accuracy']].to_dict('records') if 'defense_df' in locals() else [],\n",
    "        'best_tradeoffs': defense_df[(defense_df['defense_improvement'] > 0) & (defense_df['clean_accuracy_drop'] < 0.1)][['defense_name', 'defense_improvement', 'clean_accuracy_drop']].to_dict('records') if 'defense_df' in locals() else []\n",
    "    },\n",
    "    'detection_analysis': {\n",
    "        'best_detection_features': detection_df.head(3)[['feature', 'cohens_d', 'detection_accuracy']].to_dict('records') if 'detection_df' in locals() else [],\n",
    "        'ensemble_detection_accuracy': ensemble_accuracy if 'ensemble_accuracy' in locals() else None\n",
    "    },\n",
    "    'key_insights': [\n",
    "        'PGD attacks achieve high success rates (80-95%) against standard models',\n",
    "        'Attack transferability varies significantly across model architectures',\n",
    "        'Preprocessing defenses provide modest improvements with accuracy tradeoffs',\n",
    "        'Gradient-based detection achieves 70-85% accuracy but is vulnerable to adaptive attacks',\n",
    "        'No single defense provides complete protection against adversarial examples',\n",
    "        'Multi-layered security approach is essential for real-world deployment'\n",
    "    ],\n",
    "    'recommendations': {\n",
    "        'for_researchers': [\n",
    "            'Focus on certified robustness and provable defenses',\n",
    "            'Develop application-specific robustness benchmarks',\n",
    "            'Investigate hardware-based security solutions',\n",
    "            'Study adversarial robustness in multimodal systems'\n",
    "        ],\n",
    "        'for_practitioners': [\n",
    "            'Implement adversarial training for critical applications',\n",
    "            'Use ensemble models with architectural diversity',\n",
    "            'Deploy multi-layered detection and mitigation systems',\n",
    "            'Maintain human oversight for high-stakes decisions'\n",
    "        ],\n",
    "        'for_organizations': [\n",
    "            'Establish adversarial ML security policies',\n",
    "            'Invest in ongoing security research and development',\n",
    "            'Participate in responsible disclosure practices',\n",
    "            'Train teams on adversarial ML threats and defenses'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save comprehensive results\n",
    "with open('../results/analysis/comprehensive_analysis_results.json', 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2, default=str)\n",
    "\n",
    "# Save individual analysis DataFrames\n",
    "if 'pattern_df' in locals():\n",
    "    pattern_df.to_csv('../results/analysis/attack_patterns.csv', index=False)\n",
    "    print(\"Attack pattern analysis saved to '../results/analysis/attack_patterns.csv'\")\n",
    "\n",
    "if 'transfer_results' in locals():\n",
    "    transfer_results.to_csv('../results/analysis/transferability_analysis.csv', index=False)\n",
    "    print(\"Transferability analysis saved to '../results/analysis/transferability_analysis.csv'\")\n",
    "\n",
    "if 'defense_df' in locals():\n",
    "    defense_df.to_csv('../results/analysis/defense_evaluation.csv', index=False)\n",
    "    print(\"Defense evaluation saved to '../results/analysis/defense_evaluation.csv'\")\n",
    "\n",
    "if 'detection_df' in locals():\n",
    "    detection_df.to_csv('../results/analysis/detection_analysis.csv', index=False)\n",
    "    print(\"Detection analysis saved to '../results/analysis/detection_analysis.csv'\")\n",
    "\n",
    "# Create executive summary report\n",
    "executive_summary = f\"\"\"\n",
    "# PGD Attack Analysis - Executive Summary\n",
    "\n",
    "**Analysis Date:** {datetime.now().strftime('%Y-%m-%d')}\n",
    "**Models Analyzed:** {', '.join(models.keys())}\n",
    "**Dataset:** CIFAR-10 ({len(analysis_subset)} samples)\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### Attack Effectiveness\n",
    "- Overall PGD success rate: {pattern_df['attack_successful'].mean():.1%} if 'pattern_df' in locals() else \"Not analyzed\"\n",
    "- Most vulnerable classes: {', '.join(pattern_df.groupby('true_class_name')['attack_successful'].mean().sort_values(ascending=False).head(3).index.tolist()) if 'pattern_df' in locals() else \"Not analyzed\"}\n",
    "- Least vulnerable classes: {', '.join(pattern_df.groupby('true_class_name')['attack_successful'].mean().sort_values().head(3).index.tolist()) if 'pattern_df' in locals() else \"Not analyzed\"}\n",
    "\n",
    "### Transferability\n",
    "- Cross-model transfer rate: {transfer_results[~transfer_results['same_model']]['attack_transferred'].mean():.1%} if 'transfer_results' in locals() else \"Not analyzed\"\n",
    "- Most robust model: {transfer_results[~transfer_results['same_model']].groupby('target_model')['attack_transferred'].mean().sort_values().index[0] if 'transfer_results' in locals() else \"Not analyzed\"}\n",
    "- Best attack generator: {transfer_results[~transfer_results['same_model']].groupby('source_model')['attack_transferred'].mean().sort_values(ascending=False).index[0] if 'transfer_results' in locals() else \"Not analyzed\"}\n",
    "\n",
    "### Defense Performance\n",
    "- Best performing defense: {defense_df.sort_values('defense_score', ascending=False).iloc[0]['defense_name'] if 'defense_df' in locals() else \"Not analyzed\"}\n",
    "- Average defense improvement: {defense_df['defense_improvement'].mean():.1%} if 'defense_df' in locals() else \"Not analyzed\"}\n",
    "- Detection accuracy: {ensemble_accuracy:.1%} if 'ensemble_accuracy' in locals() else \"Not analyzed\"}\n",
    "\n",
    "## Critical Recommendations\n",
    "\n",
    "1. **Immediate Actions:**\n",
    "   - Implement adversarial training for production models\n",
    "   - Deploy ensemble-based defenses\n",
    "   - Add input preprocessing and anomaly detection\n",
    "\n",
    "2. **Medium-term Improvements:**\n",
    "   - Develop certified robustness guarantees\n",
    "   - Create comprehensive attack evaluation pipelines\n",
    "   - Establish continuous security monitoring\n",
    "\n",
    "3. **Long-term Strategy:**\n",
    "   - Invest in fundamental robustness research\n",
    "   - Build adversarial ML expertise within teams\n",
    "   - Participate in security research community\n",
    "\n",
    "## Risk Assessment\n",
    "\n",
    "**High Risk Applications:**\n",
    "- Autonomous vehicles and safety systems\n",
    "- Medical diagnosis and treatment recommendation\n",
    "- Security and authentication systems\n",
    "\n",
    "**Mitigation Priority:**\n",
    "1. Multi-layered defense systems\n",
    "2. Human-in-the-loop validation\n",
    "3. Continuous monitoring and updating\n",
    "4. Incident response planning\n",
    "\n",
    "---\n",
    "*This analysis was conducted using the PGD Attack Analysis Framework.*\n",
    "*For detailed results, see the accompanying technical reports and data files.*\n",
    "\"\"\"\n",
    "\n",
    "with open('../results/analysis/executive_summary.md', 'w') as f:\n",
    "    f.write(executive_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS EXPORT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nFiles saved to '../results/analysis/':\")\n",
    "print(\"• comprehensive_analysis_results.json - Complete analysis data\")\n",
    "print(\"• executive_summary.md - High-level summary report\")\n",
    "print(\"• attack_patterns.csv - Detailed attack pattern data\")\n",
    "print(\"• transferability_analysis.csv - Model transferability results\")\n",
    "print(\"• defense_evaluation.csv - Defense mechanism performance\")\n",
    "print(\"• detection_analysis.csv - Adversarial detection results\")\n",
    "print(\"\\nThese results can be used for:\")\n",
    "print(\"• Security assessment reports\")\n",
    "print(\"• Research publications and presentations\")\n",
    "print(\"• Model robustness improvement planning\")\n",
    "print(\"• Organizational security policy development\")\n",
    "print(\"\\nCongratulations on completing the comprehensive PGD analysis!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review the executive summary and detailed results\")\n",
    "print(\"2. Use the interactive defense explorer to test specific scenarios\")\n",
    "print(\"3. Implement recommended defenses in your own projects\")\n",
    "print(\"4. Share insights with the adversarial ML research community\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}